\chapter{Introduction} \label{ch:intro}
% objective
Grammatical error correction (GEC) is the task of automatically transforming text with potential grammatical errors into grammatically correct text. Although both input and output text are in the same language, this transformation is similar to the task of translating text from one language to another, so much of recent work has focused on applying the same techniques to GEC that have been successful for translation. While neural machine translation (NMT) techniques have generally outperformed statistical machine translation (SMT) techniques at translation tasks, the same pattern has not been the case for GEC. GEC systems trained using standard NMT methods tend to have low recall, i.e. they learn to copy instead of correcting grammatical errors; consequently, SMT systems remain more effective for GEC. The goal of this project is to improve recall using NMT for GEC.

% motivation
A key difference that sets apart GEC from MT is that part or all of the input text can be identical to the corresponding output text; not only are both texts in the same language but most input text is error-free in practice (see section \ref{subsec:error-rates} for frequency of grammatical errors in commonly used GEC datasets). An unfortunate consequence is that out-of-the-box NMT techniques result in GEC systems that learn to copy instead of correcting grammatical errors, resulting in poor recall. Perhaps this issue has been neglected in recent research due to the fact that the standard metric for the past few years is an $F_{0.5}$ score, which places twice the importance on precision as on recall; the idea behind it is that producing output that is unchanged and equally ungrammatical to the input is preferable to producing output that is changed and more ungrammatical. In this project we focus particularly on recall instead of this $F_{0.5}$ score. We hypothesise that applying a higher cost to grammatical errors during training will prevent the system from learning to copy and will improve its recall.

% results
After modifying the default training objective to relatively punish missed grammatical errors more than unnecessary edits, we successfully forced several models to learn to correct grammatical errors. In general, the greater the relative emphasis, the greater the improvement in recall. As an added bonus, we found that far from sacrificing any precision, our approach ended up improving it as well, proving our strategy an unequivocal success.

% outline
% TODO: revisit
The remainder of this document is as follows: Chapter \ref{ch:background} surveys the evolution of recent GEC work, including the development of the task-specific MaxMatch scoring framework. It outlines the methods of phrase-based SMT and NMT and reviews work that has applied these approaches to GEC. Chapter \ref{ch:methods} summarises the model architecture used in all of our experiments; training hyperparameter values, the original and modified objective function, convergence criterion, and tools used; as well as the datasets used in this work. Chapter \ref{ch:results} compares the results of baseline models against models with varying values of the novel training hyperparameter. Finally, chapter \ref{ch:conclusion} concludes with further observations and suggestions for future work.