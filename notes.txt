MSc Notes
Shubha Guha
Summer 2017


=== Setting up to use Azure on local machine ===

- Create conda env with python3

- Get Kenneth's Azure scripts
git clone https://github.com/kpu/azurehacks.git

- Install prerequisites for Kenneth's scripts
brew install parallel

- Launch virtual machine scale set
az login  # with Kenneth to accept my code
az group create -l southcentralus -n ${USER}-gec-nmt
./vmss_create.sh --resource-group ${USER}-gec-nmt --name ${USER}-scale --instance-count 1 --vm-sku Standard_NC6 --ssh-key-value ~/.ssh/id_rsa.pub --image /subscriptions/b97717d6-1424-45c1-b6e6-316241c3cd70/resourceGroups/HEAFIELD-BASE/providers/Microsoft.Compute/images/wmtbase --generate-ssh-keys
./vmss_setup.sh --resource-group ${USER}-gec-nmt --name ${USER}-scale

- Other useful Azure commands
az vmss list-instances --resource-group ${USER}-gec-nmt --name ${USER}-scale
az vmss deallocate --resource-group ${USER}-gec-nmt --name ${USER}-scale --instance-ids 0  # if deallocate command doesn't work on VM
az vmss start --resource-group ${USER}-gec-nmt --name ${USER}-scale --instance-ids 0
az vmss scale --resource-group ${USER}-gec-nmt --name ${USER}-scale --new-capacity 2


=== Data formatting ===

- Convert NUCLE, CoNNL-2013, and CoNLL-2014 datasets into parallel corpora

- Concatenate NUCLE and Lang-8

- Split train, valid, and test parallel texts


=== Train truecaser ===

perl mosesdecoder/scripts/recaser/train-truecaser.perl --model MSc/data/truecaser --corpus MSc/data/train.parallel


=== Preprocessing ===

- Truecase

- Apply BPE
    * TODO: find out what git version of subword-nmt was used to generate gec.bpe

- Replace | with <pipe>
    * TODO: find out exactly why


=== Postprocessing ===

- Replace <pipe> with |

- Remove BPE

- Detruecase


=== Running training ===

- Build dictionaries
python nematus/data/build_dictionary.py MSc/data/preprocessed-train.fr MSc/data/preprocessed-train.en

- Train
python nematus/nematus/nmt.py --datasets MSc/data/preprocessed-train.fr MSc/data/preprocessed-train.en
                              --dictionaries MSc/data/preprocessed-train.fr.json MSc/data/preprocessed-train.en.json
                              --model MSc/baseline_0.npz
                              [--saveFreq 10000]
                              --layer_normalisation
                              --use_dropout
                              --dropout_source 0.1
                              --dropout_target 0.1
                              --batch_size 60
                              --valid_datasets MSc/data/preprocessed-valid.fr MSc/data/preprocessed-valid.en
                              [--reload]

* run with `time` and `deallocate`
* default params:   --dropout_embedding 0.2
                    --dropout_hidden 0.2
                    --patience 10  # the number of times validation error > previous validation errors
                    --batch_size 80
                    --objective CE  # cross-entropy minimization

MRT params:         --objective MRT
                    --mrt_loss M2


=== Model file naming ===

baseline_ce_0       NUCLE only, without tokenization and BPE, default Nematus parameters, including cross-entropy minimization
baseline_mrt_0      + MRT objective
baseline_1          + Lang-8, + tokenization and BPE, default cross-entropy objective
baseline_2          + layer normalisation, default 0.2 embedding and hidden dropout, 0.1 source and target dropout
baseline_3          batch size 60, trained on M60 machine
baseline_4          batch size 40, trained on M60 machine
baseline_5          baseline_3 + replace | with <pipe>
                        started training around 9:45pm Wed June 28
                        338k minibatches by 12pm noon Sun July 2
                        early stop after 530k minibatches (by 1pm Tues July 4, 8056m46.192s real time)
baseline_6          K80 machine, default batch size 80
                        started training around 10:45pm Wed June 28
                        210k minibatches by 12pm noon Sun July 2
                        early stop after 370k minibatches (by 12pm Wed July 5, 8986m43.145s real time)
baseline_7          M60, batch size 60: - tokenization + truecaser, replace pipes last
                        started training around 2:30pm Sun July 2
                        early stop after 590k minibatches (before 9pm Sat July 8, 8289m6.922s real time)
                        P 0.3319, R 0.1413
weighted_ce_0
pretrained_7        final baseline_7 + MRT on M2


===================================================================================================
MEETING NOTES
===================================================================================================


=== Meeting notes 12 June 2017 ===

back translation / data augmentation using smt
LM for reranking NMT outputs (or mixing models) doesn't seem to work

what to use LM for?
- measure fluency
- Xie et al: linear combination


=== Meeting summary 23 June 2017 ===

(1) (re)do preprocessing: tokenization, BPE (ask Roman)
(2) preprocess and use validation dataset in tracking training progress
(3) use dropout (~0.1)
(4) implement script for M2 (ask Roman)
(5) modify Nematus code to use word-level weighted loss function


=== Questions for Kenneth 27 June 2017 ===

PREPROCESSING:

two different tokenizer results (Roman's NLTK tokenizer vs. Moses tokenizer included in Nematus)
- main difference seems to be escaping symbols like `&`

how to apply BPE?
- subword-nmt/learn_bpe.py learns different bpe than gec.bpe from Roman
- subword-nmt/apply_bpe.py produces new training file with few but strange differences
- is it only meant for test time?

increase number of subword units to capture morphology? could help learn to correct e.g. misspellings like `reliabiliyt` instead of `reliability`

when is detokenizer necessary?

DROPOUT:

5 dropout parameters in Nematus: --use_dropout  use dropout layer (default: False)
                                 --dropout_embedding FLOAT   dropout for input embeddings (0: no dropout) (default: 0.2)
                                 --dropout_hidden FLOAT  dropout for hidden layer (0: no dropout) (default: 0.2)
                                 --dropout_source FLOAT  dropout source words (0: no dropout) (default: 0)
                                 --dropout_target FLOAT  dropout target words (0: no dropout) (default: 0)


=== Meeting notes 27 June 2017 ===

doesn't matter what tokenizer; just be consistent and report which one

check with Roman which git version of subword-nmt he used to create gec.bpe

talk to Maxi about trying to learn morphology with more BPE (didn't work)

--layer_normalisation --use_dropout --dropout_source 0.1 --dropout_target 0.1

try smaller batch size on m60 (converge faster)

replace pipe symbols with something unique like <pipe> or &#124
do after tokenization
undo before detokenization

detokenize before evaluation on M2


=== Meeting notes 5 July 2017 ===

CE with editted words weighted extra

M2 scorer for MRT in Nematus --> need aligner for hypothesis vs reference sentences
