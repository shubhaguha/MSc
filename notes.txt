MSc Notes
Shubha Guha
Summer 2017


=== Setting up to use Azure on local machine ===

- Create conda env with python3

- Get Kenneth's Azure scripts
git clone https://github.com/kpu/azurehacks.git

- Install prerequisites for Kenneth's scripts
brew install parallel

- Launch virtual machine scale set
az login  # with Kenneth to accept my code
az group create -l southcentralus -n ${USER}-gec-nmt
./vmss_create.sh --resource-group ${USER}-gec-nmt --name ${USER}-scale --instance-count 1 --vm-sku Standard_NC6 --ssh-key-value ~/.ssh/id_rsa.pub --image /subscriptions/b97717d6-1424-45c1-b6e6-316241c3cd70/resourceGroups/HEAFIELD-BASE/providers/Microsoft.Compute/images/wmtbase --generate-ssh-keys
./vmss_setup.sh --resource-group ${USER}-gec-nmt --name ${USER}-scale

- Other useful Azure commands
az vmss list-instances --resource-group ${USER}-gec-nmt --name ${USER}-scale
az vmss deallocate --resource-group ${USER}-gec-nmt --name ${USER}-scale --instance-ids 0  # if deallocate command doesn't work on VM
az vmss start --resource-group ${USER}-gec-nmt --name ${USER}-scale --instance-ids 0
az vmss scale --resource-group ${USER}-gec-nmt --name ${USER}-scale --new-capacity 2


=== Data formatting ===

- Convert NUCLE, CoNNL-2013, and CoNLL-2014 datasets into parallel corpora

- Concatenate NUCLE and Lang-8

- Split train, valid, and test parallel texts


=== Train truecaser ===

perl mosesdecoder/scripts/recaser/train-truecaser.perl --model MSc/data/truecaser --corpus MSc/data/train.parallel


=== Preprocessing ===

- Truecase

- Apply BPE
    * I used the original simple BPEs. This commit should work: fb526f1b007420d6b76c34417ab42d47ffd2d850 (subword-nmt)

- Replace | with <pipe>
    * because special character in Nematus


=== Postprocessing ===

- Replace <pipe> with |

- Remove BPE

- Detruecase


=== Running training ===

- Build dictionaries
python nematus/data/build_dictionary.py MSc/data/preprocessed-train.fr MSc/data/preprocessed-train.en

- Train
python nematus/nematus/nmt.py --datasets MSc/data/preprocessed-train.fr MSc/data/preprocessed-train.en
                              --dictionaries MSc/data/preprocessed-train.fr.json MSc/data/preprocessed-train.en.json
                              --model MSc/baseline_0.npz
                              [--saveFreq 10000]
                              --layer_normalisation
                              --use_dropout
                              --dropout_source 0.1
                              --dropout_target 0.1
                              --batch_size 60
                              --valid_batch_size 60
                              --valid_datasets MSc/data/preprocessed-valid.fr MSc/data/preprocessed-valid.en
                              [--reload]

* run with `time` and `deallocate`

default params:   --dropout_embedding 0.2
                  --dropout_hidden 0.2
                  --patience 10  # the number of times validation error > previous validation errors
                  --objective CE  # cross-entropy minimization

thesis param:     --edit_weight 2,3,4,5 [default 1]

MRT params:       --objective MRT
                  --mrt_loss M2


=== Model naming ===

baseline_ce_0       NUCLE only, without tokenization or BPE, default Nematus parameters, cross-entropy minimization
baseline_mrt_0      + MRT objective
baseline_1          + Lang-8, + tokenization and BPE, default cross-entropy objective
baseline_2          + layer normalisation, default 0.2 embedding and hidden dropout, 0.1 src and trgt dropout
baseline_3          batch size 60, trained on M60 machine
baseline_4          batch size 40, trained on M60 machine
baseline_5          baseline_3 + replace | with <pipe>
                        started training around 9:45pm Wed June 28
                        338k minibatches by 12pm noon Sun July 2
                        early stop after 530k minibatches (by 1pm Tues July 4, 8056m46.192s real time)
baseline_6          K80 machine, default batch size 80
                        started training around 10:45pm Wed June 28
                        210k minibatches by 12pm noon Sun July 2
                        early stop after 370k minibatches (by 12pm Wed July 5, 8986m43.145s real time)
baseline_7          M60, batch size 60: - tokenization + truecaser, replace pipes last
                        started training around 2:30pm Sun July 2
                        early stop after 590k minibatches (before 9pm Sat July 8, 8289m6.922s real time)
                        P 0.3319, R 0.1413
baseline_8 *        same as baseline_7
                        started training around 6pm Wed Aug 9
                        patience 5: early stop after 410k minibatches (by 5:30pm Sun Aug 13)
                        restarted training around 6:30pm Sun Aug 13
                        patience 10:
                        ssh -p 50008 13.84.189.224   #id 21
baseline_9 *        same as baseline_7 and baseline_8
                        started training around 10:30pm Thurs Aug 10
                        ssh -p 50007 13.84.189.224   #id 24
edit_weight_2       M60, batch size 60, edit weight 2 (commit 48fb87c)
                        started training around 2pm Wed July 19
                        early stop after 400k minibatches (before 9pm Sun July 23, 3847m50.989s real time)
                        P 0.1546, R 0.2630
edit_weight_3       edit weight 3 (commit 48fb87c)
                        started training around 2pm Wed July 19
                        early stop after 600k minibatches (before 5pm Tue July 25, 6668m41.838s real time)
                        P 0.1591, R 0.2520
edit_weight_3_x2    same as edit_weight_3
                        started training around 3:30pm Sat July 29
                        early stop after 510 minibatches (by 9pm Thur Aug 3, 5649m17.291s real time)
                        P 0.1556, R 0.2576
edit_weight_4       edit weight 4 (commit 48fb87c)
                        started training around 2pm Wed July 19
                        early stop after 420k minibatches (before 9pm Sun July 23, 3897m54.745s real time)
                        P 0.1566, R 0.2665
edit_weight_5       edit weight 5 (commit 48fb87c)
                        started training around 2pm Wed July 19
                        early stop after 380k minibatches (before 2pm Sun July 23, 5316m30.417s real time)
                        P 0.1582, R 0.2704
edit_vectors_2 *    edit vectors and edit weight 2 (commit 842c2a5)
                        started training around 12:45am Thurs Aug 10
                        ssh -p 50000 13.84.189.224   #id 0
                        patience 5: early stop after 340k minibatches (by 11:30am Sun Aug 13)
                        restarted training around 12:30pm Sun Aug 13
                        patience 10:
edit_vectors_3 *    edit vectors and edit weight 3 (commit 842c2a5)
                        started training around 12:45am Thurs Aug 10
                        ssh -p 50001 13.84.189.224   #id 4
                        patience 5: 
edit_vectors_4      edit vectors and edit weight 4 (commit 842c2a5)
                        started training around 12:45am Thurs Aug 10
                        patience 5: early stop after 300k minibatches (by 1:45am Sun Aug 13)
                        restarted training around 2:15am Sun Aug 13
                        patience 10: early stop after 350k minibatches (at 1:50pm exactly Sun Aug 13)
                        P 0.3984, R 0.2294
edit_vectors_5 *    edit vectors and edit weight 5 (commit 842c2a5)
                        started training around 12:45am Thurs Aug 10
                        ssh -p 50003 13.84.189.224   #id 7
                        patience 5: early stop after 350k minibatches (by 1:30pm Sun Aug 13)
                        restarted training around 1:45pm Sun Aug 13
                        patience 10: 
edit_vectors_6 *    edit vectors and edit weight 6 (commit 842c2a5)
                        started training around 12:45am Thurs Aug 10
                        ssh -p 50004 13.84.189.224   #id 9
                        patience 5: 
edit_vectors_1 *    edit vectors and edit weight 1 (commit 842c2a5)
                        started training around 12:45am Thurs Aug 10
                        ssh -p 50005 13.84.189.224   #id 13
                        patience 5: early stop after 320k minibatches (by 11:30am Sun Aug 13)
                        restarted training around 11:30am Sun Aug 13
                        patience 10:
edit_vectors_0      edit vectors and edit weight 0 (commit 842c2a5)
                        started training around 12:45am Thurs Aug 10
                        patience 5: early stop after 160k minibatches (by 4:30pm Fri Aug 11)
                        restarted training around 6:15pm Fri Aug 11
                        patience 10: early stop after 210k minibatches (by 11am Sat Aug 12)
                        P 0.0805, R 0.0248
edit_vectors_-1     edit vectors and edit weight -1 (commit 842c2a5), lrate 0.00001
pretrained_7        final baseline_7 + MRT on M2


===================================================================================================
MEETING NOTES
===================================================================================================


=== Meeting notes 12 June 2017 ===

back translation / data augmentation using smt
LM for reranking NMT outputs (or mixing models) doesn't seem to work

what to use LM for?
- measure fluency
- Xie et al: linear combination


=== Meeting summary 23 June 2017 ===

(1) (re)do preprocessing: tokenization, BPE (ask Roman)
(2) preprocess and use validation dataset in tracking training progress
(3) use dropout (~0.1)
(4) implement script for M2 (ask Roman)
(5) modify Nematus code to use word-level weighted loss function


=== Questions for Kenneth 27 June 2017 ===

PREPROCESSING:

two different tokenizer results (Roman's NLTK tokenizer vs. Moses tokenizer included in Nematus)
- main difference seems to be escaping symbols like `&`

how to apply BPE?
- subword-nmt/learn_bpe.py learns different bpe than gec.bpe from Roman
- subword-nmt/apply_bpe.py produces new training file with few but strange differences
- is it only meant for test time?

increase number of subword units to capture morphology? could help learn to correct e.g. misspellings like `reliabiliyt` instead of `reliability`

when is detokenizer necessary?

DROPOUT:

5 dropout parameters in Nematus: --use_dropout  use dropout layer (default: False)
                                 --dropout_embedding FLOAT   dropout for input embeddings (0: no dropout) (default: 0.2)
                                 --dropout_hidden FLOAT  dropout for hidden layer (0: no dropout) (default: 0.2)
                                 --dropout_source FLOAT  dropout source words (0: no dropout) (default: 0)
                                 --dropout_target FLOAT  dropout target words (0: no dropout) (default: 0)


=== Meeting notes 27 June 2017 ===

doesn't matter what tokenizer; just be consistent and report which one

check with Roman which git version of subword-nmt he used to create gec.bpe

talk to Maxi about trying to learn morphology with more BPE (didn't work)

--layer_normalisation --use_dropout --dropout_source 0.1 --dropout_target 0.1

try smaller batch size on m60 (converge faster)

replace pipe symbols with something unique like <pipe> or &#124
do after tokenization
undo before detokenization

detokenize before evaluation on M2


=== Meeting notes 5 July 2017 ===

CE with editted words weighted extra

M2 scorer for MRT in Nematus --> need aligner for hypothesis vs reference sentences


=== Meeting notes 28 July 2017 ===

had in mind separate aligner (M2), naive
- pass in 3rd file?
- munge

run with this but provide evidence that dec_alphas aligns to the right token (generally in MT, looks back one token from the one expected)

stacked graphs (precision and recall on y axis, edit weight on x axis)

argue that converges faster?


=== Meeting notes 9 August 2017 ===

try edit weights 1, 0, -1

calculate training loss for one sentence as sanity check
